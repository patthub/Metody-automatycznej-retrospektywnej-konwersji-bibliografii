# -*- coding: utf-8 -*-
"""bibliographic_record_enricher

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p0IToHvrR0WeCnXzhiLaW8LMTtkc2ghZ
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
import regex as re
import json
import requests
import Levenshtein


def read_file(path):
	with open(path, "r", encoding = "utf-8") as f:
		data = f.readlines()
	return data


def get_data(k: str, v: str) -> list:

    BASE_BN_URL = "http://data.bn.org.pl/api/institutions/bibs.json?marc=245a"

    response_main = {}
    url = f"http://data.bn.org.pl/api/institutions/bibs.json?marc=245a+{v}"

    responses = []
    try:
        while url:
            url = requests.get(url)
            if url.status_code == 200:
                url = url.json()
                responses.append(url)
                url = url["nextPage"]
                print(f"Downloading: {url}, {v}")
            else:
                print("Error while accessing API")
        print("Download complete")
        response = responses[0]
        print(url)
        if len(response["bibs"]) > 0:

            response = dict({(k , v) for k, v in response["bibs"][0].items() if k in["author", "title", "id", "genre"]})
            response_main[k] = (v, response)
            return response_main
        else:
            print("0 records found")
    except IndexError:
        pass

def get_data_for_bibliographic_source(v="No data") -> list:
    BASE_BN_URL = "http://data.bn.org.pl/api/institutions/bibs.json?marc=245a"
    response_main = {}
    if len(v) < 2 or v is None:
        v = "Error, data not found"

    url = f"https://data.bn.org.pl/api/institutions/bibs.json?title={v}"
    responses = []
    max_pages = 10  # Maksymalna liczba stron, które chcemy przetworzyć
    current_page = 0

    try:
        while url and current_page < max_pages:
            response = requests.get(url)

            if response.status_code == 200:
                response_json = response.json()
                responses.append(response_json)
                url = response_json.get("nextPage")
                current_page += 1
                print(f"Przetwarzanie strony {current_page}: {url}")
            else:
                print("Error while accessing API")
                break

        final_responses = []
        for response in responses:
            print(response)
            if len(response["bibs"]) > 0:
                final_responses.append(response)
            else:
                print("0 records found")
        print(final_responses)
        return final_responses


    except IndexError:
        print("Błąd: IndexError")
        return []



def magic_add_to_dict(key, value, dest):
    if key in dest:
        dest[key].append(value)
    else:
        dest[key] = [value]



dfrom jellyfish import jaro_winkler_similarity

def create_bibliographic_source_record(pbl_record, author_name):
    try:
        # Pobierz dane bibliograficzne
        temp = get_data_for_bibliographic_source(pbl_record.get("Name"))
        if temp and temp != "*":
            hits = []
            # Normalizacja nazwy autora
            author_normalized = ''.join(e.lower() for e in author_name if e.isalnum())

            # Ograniczenie do maksymalnie 10 stron wyników
            for elem in temp[:10]:
                for x in elem["bibs"]:
                    for field in x["marc"]["fields"]:
                        if "100" in field:  # Pole MARC 700 przechowuje dane o autorach
                            original_author = field["100"]["subfields"][0].get("a", "")
                            original_author_normalized = ''.join(e.lower() for e in original_author if e.isalnum())
                            # Oblicz podobieństwo Jaro-Winkler
                            jaro_winkler_score = jaro_winkler_similarity(author_normalized, original_author_normalized)
                            hits.append({
                                "author_for_comparison": original_author_normalized,
                                "whole_rec": x["marc"]["fields"],
                                "JaroWinkler": (jaro_winkler_score, author_normalized, original_author_normalized)
                            })
                            # Przerwij, jeśli znajdziesz idealne dopasowanie
                            if jaro_winkler_score == 1.0:
                                break

            # Filtruj i sortuj trafienia
            quality_hits = [hit for hit in hits if hit["JaroWinkler"][0] >= 0.9]
            quality_hits.sort(key=lambda x: x["JaroWinkler"][0], reverse=True)

            if quality_hits:
                final_hit = quality_hits[0]
                final_hit_filtered = {}
                for field in final_hit.get("whole_rec"):
                    if "009" in field:
                        final_hit_filtered["bn_id"] = field.get("009")
                    elif "100" in field:
                        final_hit_filtered["author"] = field.get("100").get("subfields")[0].get("a")
                    elif "245" in field:
                        final_hit_filtered["bn_title"] = field.get("245").get("subfields")[0].get("a")
                    elif "246" in field:
                        final_hit_filtered["246_title"] = field.get("246").get("subfields")[0].get("a")
                    elif "380" in field:
                        magic_add_to_dict("bn_form_of_work", field.get("380").get("subfields")[0].get("a"), final_hit_filtered)
                    elif "650" in field:
                        magic_add_to_dict("bn_subjects", field.get("650").get("subfields")[0].get("a"), final_hit_filtered)
                    elif "655" in field:
                        magic_add_to_dict("bn_genre_form", field.get("655").get("subfields")[0].get("a"), final_hit_filtered)
                    elif "700" in field:
                        magic_add_to_dict(
                            "bn_secondary_author",
                            (field.get("700").get("subfields")[0].get("a"), field.get("700").get("subfields")[-1].get("e")),
                            final_hit_filtered
                        )
                pbl_record["BN_INFO"] = final_hit_filtered
            else:
                print("Brak trafień o wystarczającej jakości")

            return pbl_record
        else:
            print("Brak rekordu")
    except AttributeError as e:
        print(f"Błąd atrybutu: {e}")
    except TypeError as e:
        print(f"Błąd typu: {e}")
    except Exception as e:
        print(f"Nieoczekiwany błąd: {e}")